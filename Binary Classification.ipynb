{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Binary Classification Example\n",
    "\n",
    "*Supervised* machine learning techniques involve training a model to operate on a set of *features* and predict a *label* using a dataset that includes some already-known label values. The training process *fits* the features to the known labels to define a general function that can be applied to new features for which the labels are unknown, and predict them. You can think of this function like this, in which ***y*** represents the label we want to predict and ***x*** represents the features the model uses to predict it.\n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "*Classification* is a form of supervised machine learning in which you train a model to use the features (the ***x*** in our function) to predict a label (***y***) that represents one of a number of possible classes. The simplest form of classification is *binary* classification, in which the label is 0 or 1, representing one of two classes; for example, \"True\" or \"False\"; \"Internal\" or \"External\"; \"Profitable\" or \"Non-Profitable\"; and so on. We'll focus on binary classification in this example, but many of the same principles apply to *multiclass* classification in which there are more than two possible classes.\n",
    "\n",
    "## Preparing Data for Model Training\n",
    "Let's start by looking at some data. Run the following cell to load a CSV file into a **pandas** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training dataset\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data consists of diagnostic information about some patients who have been tested for diabetes. Scroll to the right if necessary, and note that the final column in the dataset (**Diabetic**) contains the value ***0*** for patients who tested negative for diabetes, and ***1*** for patients to tested positive. This is the label that we will train our mode to predict; most of the other columns (**Pregnancies**,**PlasmaGlucose**,**DiastolicBloodPressure**, and so on) are the features we will use to predict the **Diabetic** label. Let's separate those - we'll call the features ***X*** and the label ***Y***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X, Y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset includes known values for the label, so we can use this to train a classifier so that it finds a statistical relationship between the features and the label value; but how will we know if our model is any good? How do we know it will predict correctly when we use it with new data that it wasn't trained with? Well, we can take advantage of the fact we have a large dataset with known label values, use only some of it to train the model, and hold back some to test the trained model - enabling us to compare the predicted labels with the already known labels in the test set.\n",
    "\n",
    "In Python, the **scikit-learn** package contains a large number of functions we can use to build a machine learning model - including a **train_test_split** function that ensures we get a statistically random split of training and test data. We'll use that to split the data into 65% for training and hold back 35% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data 65%-35% into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.35, random_state=0)\n",
    "\n",
    "print ('Training Set: %d, Test Set: %d \\n' % (X_train.size, X_test.size))\n",
    "\n",
    "print(\"Sample of features and labels:\")\n",
    "# Take a look at the first 10 training features and corresponding labels\n",
    "for n in range(0,9):\n",
    "    print(X_train[n], Y_train[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output from the previous cell, you can see the number of training and test cases, and a sample of the first ten features (in \\[square brackets\\] with the corresponding known label.\n",
    "\n",
    "## Training the Model\n",
    "OK, now we're ready to train our model by fitting the training features (**X_train**) to the training labels (**Y_train**). There are various algorithms we can use to train the model. In this example, we'll use *Logistic Regression*, which is a well-established algorithm for classification. In addition to the training features and labels, we'll need to set a *regularization* parameter. This is used to counteract any bias in the sample, and help the model generalize well by avoiding *overfitting* the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.01\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "clf1 = LogisticRegression(C=1/reg).fit(X_train, Y_train)\n",
    "print (clf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluating the Model\n",
    "So now we've trained the model using the training data, we can use the test data we held back to evaluate how well it predicts. Again, **scikit-learn** can help us do this. Let's start by using the model to predict labels for our test set, and compare the predicted labels to the known labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf1.predict(X_test)\n",
    "print('Predicted labels: ', predictions)\n",
    "print('Actual labels: ' ,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays of labels are too long to be displayed in the notebook output, so we can only compare a few values. Even if we printed out all of the predicted and actual labels, there are too many of them to make this a sensible way to evaluate the model. Fortunately, **scikit-learn** has a few more tricks up its sleeve, and it provides some metrics that we can use to evaluate the model.\n",
    "\n",
    "The most obvious thing you might want to do is to check the *accuracy* of the predictions - in simple terms, what proportion of the labels did the model predict correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ', accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is returned as a decimal value - a value of 1.0 would mean that the model got 100% of the predictions right; while an accuracy of 0.0 is, well, pretty useless!\n",
    "\n",
    "Accuracy seems like a sensible metric to evaluate (and to a certain extent it is), but you need to be careful about drawing too many conclusions from the accuracy of a classifier. Remember that it's simply a measure of how many cases were predicted correctly. Suppose only 3% of the population is diabetic. You could create a classifier that always just predicts 0, and it would be 97% accurate - but not terribly helpful in identifying patients with diabetes!\n",
    "\n",
    "Fortunately, there are some other metrcs that reveal a little more about how our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report includes the following metrics:\n",
    "* *Precision*: The proportion of *positive* (1) predictions that were in fact positive.\n",
    "* *Recall*: The proportion of actual positive cases that the classifier correctly identified.\n",
    "* *F1-Score*: An average metric that takes both precision and recall into account.\n",
    "* *Support*: A weighted average of prevelance for the two classes.\n",
    "\n",
    "The precision and recall metrics are derived from four core metrics:\n",
    "* *True Positives*: The predicted label and the actual label are both 1.\n",
    "* *False Positives*: The predicted label is 1, but the actual label is 0.\n",
    "* *False Negatives*: The predicted label is 0, but the actual label is 1.\n",
    "* *True Negatives*: The predicted label and the actual label are both 0.\n",
    "\n",
    "These metrics are generally shown together as a *confusion matrix*, which takes the following form:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>TN</td><td>FP</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>FN</td><td>TP</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "In Python, you can use the **sklearn.metrics.confusion_matrix** function to find these values for a trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "print (tn,fp,'\\n',fn,tp,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we've considered the predictions from the model as being either a 1 or a 0. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logisitic regression, are based on *probability*; so what actually gets predicted by a binary classifier is the probability that the label is true (**P(y)**) and the probability that the label is false (1 - **P(Y)**). A threshold value of 0.5 is used to decide whether the predicted label is a 1 (*P(Y) > 0.5*) or a 0 (*P(Y) <= 0.5*). You can use the **predict_proba** method to see the probability pairs for each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_scores = clf1.predict_proba(X_test)\n",
    "print(Y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilties are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the *true positive rate* (which is another name for recall) and the *false positive rate* for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a *received operator characteristic (ROC) chart*, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 4), dpi=75)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\n",
    "\n",
    "The area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(Y_test,Y_scores[:,1])\n",
    "print('AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ROC curve and its AUC indicate that the model performs better than a random guess.\n",
    "\n",
    "## Persisting and Using a Model\n",
    "Now that we have a reasonably useful trained model, we can save it for use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print (\"Exporting the model to model.pkl\")\n",
    "f = open('data/model.pkl', 'wb')\n",
    "pickle.dump(clf1, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then when we have some new observations for which the lable is unknown, we can load the model and use it to predict values for the unknown label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pickle file\n",
    "print(\"Importing the model from model.pkl\")\n",
    "f2 = open('data/model.pkl', 'rb')\n",
    "clf2 = pickle.load(f2)\n",
    "\n",
    "# predict on a new sample\n",
    "X_new = [[2,180,74,24,21,23.9091702,1.488172308,22]]\n",
    "print ('New sample: {}'.format(X_new))\n",
    "\n",
    "pred = clf2.predict(X_new)\n",
    "print('Predicted class is {}'.format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learing More\n",
    "\n",
    "Check out the scikit-learn documentation at http://scikit-learn.org/stable/documentation.html."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
